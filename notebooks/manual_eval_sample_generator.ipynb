{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "#collecting contracts name\n",
    "dataset='smartbugs/data_analysis/'\n",
    "results_path='../results/'\n",
    "contracts_path=os.path.join(results_path,dataset,'contracts.csv')\n",
    "storage_path=os.path.join(results_path,dataset,'manual_analysis_random_samples/')\n",
    "patches_path=os.path.join(results_path,dataset,'all_patches_stats.csv')\n",
    "\n",
    "\n",
    "patches=pd.read_csv(patches_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sGuard\n",
      "['mycontract.sol' 'reentrance.sol' 'simple_dao.sol'\n",
      " '0x7541b76cb60f4c60af330c208b0623b7f54bf615.sol'\n",
      " '0xcead721ef5b11f1a7b530171aab69b16c5e66b6e.sol'\n",
      " '0x7a8721a9d64c74da899424c1b52acbf58ddc9782.sol' 'phishable.sol'\n",
      " 'reentrancy_simple.sol' '0x01f8c4e3fa3edeb29e514cba738d87ce8c091d3f.sol'\n",
      " '0xbaf51e761510c1a11bf48dd87c0307ac8a8c8a4f.sol']\n",
      "------------------------------------\n",
      "sGuardPlus\n",
      "['0x7541b76cb60f4c60af330c208b0623b7f54bf615.sol'\n",
      " '0xf015c35649c82f5467c9c74b7f28ee67665aad68.sol'\n",
      " '0x4e73b32ed6c35f570686b89848e5f39f20ecc106.sol' 'reentrancy_dao.sol'\n",
      " '0x7b368c4e805c3870b6c49a3f1f49f69af8662cf3.sol' 'phishable.sol'\n",
      " '0x52d2e0f9b01101a59b38a3d05c80b7618aeed984.sol'\n",
      " 'overflow_simple_add.sol' 'integer_overflow_mul.sol'\n",
      " '0xb7c5c5aa4d42967efe906e1b66cb8df9cebf04f7.sol']\n",
      "------------------------------------\n",
      "SmartFix\n",
      "['0x8c7777c45481dba411450c228cb692ac3d550344.sol' 'phishable.sol'\n",
      " '0x941d225236464a25eb18076df7da6a91d0f95e9e.sol'\n",
      " 'wallet_02_refund_nosub.sol'\n",
      " '0xb93430ce38ac4a6bb47fb1fc085ea669353fd89e.sol'\n",
      " 'modifier_reentrancy.sol' 'reentrancy_simple.sol' 'reentrancy_bonus.sol'\n",
      " 'integer_overflow_multitx_onefunc_feasible.sol' 'simple_dao.sol']\n",
      "------------------------------------\n",
      "SolGPT\n",
      "['0x07f7ecb66d788ab01dc93b9b71a88401de7d0f2e_3round.sol'\n",
      " '0x4b71ad9c1a84b9b643aa54fdd66e2dec96e8b152_1round.sol'\n",
      " '0x7541b76cb60f4c60af330c208b0623b7f54bf615_3round.sol'\n",
      " 'integer_overflow_minimal_1round.sol' 'etherstore_3round.sol'\n",
      " 'reentrancy_dao_4round.sol' 'roulette_4round.sol'\n",
      " '0xcead721ef5b11f1a7b530171aab69b16c5e66b6e_3round.sol'\n",
      " '0xb11b2fed6c9354f7aa2f658d3b4d7b31d8a13b77_3round.sol'\n",
      " 'wallet_02_refund_nosub_2round.sol']\n",
      "------------------------------------\n",
      "TIPS\n",
      "['0xe894d54dca59cb53fe9cbc5155093605c7068220U1.sol'\n",
      " '0x3a0e9acd953ffc0dd18d63603488846a6b8b2b01U1.sol'\n",
      " '0x93c32845fae42c83a70e5f06214c8433665c2ab5R1.sol'\n",
      " '0x7d09edb07d23acb532a82be3da5c17d9d85806b4U2.sol'\n",
      " '0x01f8c4e3fa3edeb29e514cba738d87ce8c091d3fR1.sol' 'phishable.sol'\n",
      " '0xbaf51e761510c1a11bf48dd87c0307ac8a8c8a4fR1.sol'\n",
      " '0x7a8721a9d64c74da899424c1b52acbf58ddc9782R1.sol'\n",
      " '0x23a91059fdc9579a9fbd0edc5f2ea0bfdb70deb4R1.sol'\n",
      " '0x07f7ecb66d788ab01dc93b9b71a88401de7d0f2eU1.sol']\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Generate 10 random samples per source code tool\n",
    "source_code_tools = [\n",
    "    'sGuard',\n",
    "    'sGuardPlus',\n",
    "    'SmartFix',\n",
    "    \"SolGPT\",\n",
    "    'TIPS',\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "for tool in source_code_tools:\n",
    "    print(tool)\n",
    "    # Fixed the syntax error: use parentheses around each condition\n",
    "    patches_tool = patches[(patches['Tool'] == tool) & (patches['mitigates'] == 'yes')]\n",
    "    \n",
    "    # Get 10 random samples or all if less than 10 are available\n",
    "    if len(patches_tool) > 10:\n",
    "        patches_sample = patches_tool.sample(10, random_state=42)\n",
    "    else:\n",
    "        patches_sample = patches_tool\n",
    "    \n",
    "    # Save the sampled patches to CSV\n",
    "    patches_sample.to_csv(os.path.join(storage_path, tool + '_patches.csv'), index=False)\n",
    "    \n",
    "    # Print contract names for verification\n",
    "    print(patches_sample['Patch'].values)\n",
    "    print('------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "access_control\n",
      "['mycontract.sol' 'simple_suicide.sol'\n",
      " 'incorrect_constructor_name2_4round.sol'\n",
      " 'wallet_02_refund_nosub_2round.sol' 'simple_suicide.sol'\n",
      " 'mycontract_2round.sol' 'proxy_4round.sol'\n",
      " 'incorrect_constructor_name1_4round.sol' 'wallet_02_refund_nosub.sol'\n",
      " 'mycontract_3round.sol']\n",
      "------------------------------------\n",
      "arithmetic\n",
      "['tokensalechallenge.sol' 'integer_overflow_mul.sol'\n",
      " 'integer_overflow_mul_4round.sol' 'integer_overflow_add.sol'\n",
      " 'token_1round.sol'\n",
      " 'integer_overflow_multitx_multifunc_feasible_3round.sol'\n",
      " 'integer_overflow_add_3round.sol' 'integer_overflow_mul_2round.sol'\n",
      " 'integer_overflow_mapping_sym_1_1round.sol' 'timelock.sol']\n",
      "------------------------------------\n",
      "bad_randomness\n",
      "['old_blockhash_3round.sol' 'old_blockhash_1round.sol'\n",
      " 'old_blockhash_2round.sol' 'blackjack_3round.sol' 'etheraffle_3round.sol'\n",
      " 'etheraffle_4round.sol' 'etheraffle_1round.sol']\n",
      "------------------------------------\n",
      "denial_of_service\n",
      "[]\n",
      "------------------------------------\n",
      "front_running\n",
      "[]\n",
      "------------------------------------\n",
      "other\n",
      "['crypto_roulette_2round.sol' 'crypto_roulette_4round.sol'\n",
      " 'crypto_roulette_1round.sol' 'crypto_roulette_3round.sol'\n",
      " 'name_registrar_1round.sol' 'name_registrar_3round.sol'\n",
      " 'name_registrar_2round.sol' 'name_registrar_4round.sol']\n",
      "------------------------------------\n",
      "reentrancy\n",
      "['reentrance.sol' '0x7a8721a9d64c74da899424c1b52acbf58ddc9782.sol'\n",
      " '0x96edbe868531bd23a6c05e9d0c424ea64fb1b78b_4round.sol'\n",
      " '0x941d225236464a25eb18076df7da6a91d0f95e9e_2round.sol' 'reentrance.sol'\n",
      " '0x93c32845fae42c83a70e5f06214c8433665c2ab5_4round.sol'\n",
      " '0xb5e1b1ee15c6fa0e48fce100125569d430f1bd12_1round.sol'\n",
      " 'reentrancy_dao_1round.sol'\n",
      " '0x23a91059fdc9579a9fbd0edc5f2ea0bfdb70deb4_1round.sol'\n",
      " '0x96edbe868531bd23a6c05e9d0c424ea64fb1b78bR1.sol']\n",
      "------------------------------------\n",
      "short_addresses\n",
      "[]\n",
      "------------------------------------\n",
      "time_manipulation\n",
      "['ether_lotto_3round.sol' 'ether_lotto_2round.sol'\n",
      " 'ether_lotto_1round.sol' 'roulette_4round.sol' 'roulette_3round.sol'\n",
      " 'roulette_2round.sol' 'roulette_1round.sol']\n",
      "------------------------------------\n",
      "unchecked_low_level_calls\n",
      "['0x4051334adc52057aca763453820cb0e045076ef3_2round.sol'\n",
      " '0xb11b2fed6c9354f7aa2f658d3b4d7b31d8a13b77.sol'\n",
      " '0xbebbfe5b549f5db6e6c78ca97cac19d1fb03082c_4round.sol'\n",
      " '0x4051334adc52057aca763453820cb0e045076ef3_3round.sol'\n",
      " '0xa1fceeff3acc57d257b917e30c4df661401d6431_1round.sol'\n",
      " '0xbaa3de6504690efb064420d89e871c27065cdd52_1round.sol'\n",
      " '0x4b71ad9c1a84b9b643aa54fdd66e2dec96e8b152_2round.sol'\n",
      " '0x2972d548497286d18e92b5fa1f8f9139e5653fd2_3round.sol'\n",
      " '0x52d2e0f9b01101a59b38a3d05c80b7618aeed984.sol'\n",
      " '0x4b71ad9c1a84b9b643aa54fdd66e2dec96e8b152_3round.sol']\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "vulnerabilities = patches['Category'].unique()\n",
    "binary_tools = [ 'Elysium', 'SmartShield']\n",
    "\n",
    "for vuln in vulnerabilities:\n",
    "    print(vuln)\n",
    "    # remove the patches from binary tools\n",
    "    patches = patches[~patches['Tool'].isin(binary_tools)]\n",
    "    patches_population = patches[(patches['Category'] == vuln) & (patches['mitigates'] == 'yes')]\n",
    "    \n",
    "    # Get 10 random samples or all if less than 10 are available\n",
    "    if len(patches_population) > 10:\n",
    "        patches_sample = patches_population.sample(10, random_state=42)\n",
    "    else:\n",
    "        patches_sample = patches_population\n",
    "    \n",
    "    # Save the sampled patches to CSV\n",
    "    patches_sample.to_csv(os.path.join(storage_path, vuln + '_patches.csv'), index=False)\n",
    "    \n",
    "    # Print contract names for verification\n",
    "    print(patches_sample['Patch'].values)\n",
    "    print('------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the diff path to the patches\n",
    "\n",
    "for tool in source_code_tools:\n",
    "    # read the patches\n",
    "    patches_tool = pd.read_csv(os.path.join(storage_path, tool + '_patches.csv'))\n",
    "    for patch_path in patches_tool['patch_link'].values:\n",
    "        diff_path = patch_path.replace(\".sol\", \".diff\")\n",
    "        # add diff path to patches_tool in the same row as patch_path\n",
    "        patches_tool.loc[patches_tool['patch_link'] == patch_path, 'diff_link'] = diff_path\n",
    "        # order the columns\n",
    "        patches_tool = patches_tool[['Patch', 'Original', 'Category', 'Tool', 'DIFF', 'COMP', 'Detected', 'Fixed', 'Consistent', 'sanity_check', 'mitigates', 'patch_link', 'original_link', 'diff_link', 'exploit_link']]\n",
    "    # updat csv\n",
    "    patches_tool.to_csv(os.path.join(storage_path, tool + '_patches.csv'), index=False)\n",
    "\n",
    "for vuln in vulnerabilities:\n",
    "    # read the patches\n",
    "    patched_vuln = pd.read_csv(os.path.join(storage_path, vuln + '_patches.csv'))\n",
    "    for patch_path in patched_vuln['patch_link'].values:\n",
    "        diff_path = patch_path.replace(\".sol\", \".diff\")\n",
    "        # add diff path to patched_vuln in the same row as patch_path\n",
    "        patched_vuln.loc[patched_vuln['patch_link'] == patch_path, 'diff_link'] = diff_path\n",
    "        # order the columns\n",
    "        patched_vuln = patched_vuln[['Patch', 'Original', 'Category', 'Tool', 'DIFF', 'COMP', 'Detected', 'Fixed', 'Consistent', 'sanity_check', 'mitigates', 'patch_link', 'original_link', 'diff_link', 'exploit_link']]\n",
    "    # updat csv\n",
    "    patched_vuln.to_csv(os.path.join(storage_path, vuln + '_patches.csv'), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code used to generate .diff files. Should be run after formatting with prettier-solidity.\n",
    "** diffs already included in the repo, no need to run **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def read_file(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return f.readlines()\n",
    "\n",
    "def diff_files(file1, file2, file1_path, file2_path):\n",
    "    formatted1 = read_file(file1)\n",
    "    formatted2 = read_file(file2)\n",
    "\n",
    "    diff = difflib.unified_diff(\n",
    "        formatted1, formatted2,\n",
    "        fromfile=file1_path, tofile=file2_path,\n",
    "        lineterm=''\n",
    "    )\n",
    "\n",
    "    return '\\n'.join(diff)\n",
    "\n",
    "def patch_path(path):\n",
    "    github_prefix = \"https://github.com/ASSERT-KTH/RepairComp/blob/main/results/\"\n",
    "\n",
    "    dir_prefix = \"../results/\"\n",
    "\n",
    "    path = path.replace(github_prefix, dir_prefix)\n",
    "    return path\n",
    "\n",
    "def og_path(path):\n",
    "    github_prefix = \"https://github.com/smartbugs/smartbugs-curated/tree/main/\"\n",
    "    dir_prefix = \"../../smartbugs-curated/\"\n",
    "    path = path.replace(github_prefix, dir_prefix)\n",
    "    return path\n",
    "\n",
    "def generate_diffs():\n",
    "    for vuln in vulnerabilities:\n",
    "        # read the patches\n",
    "        patches_vuln = pd.read_csv(os.path.join(storage_path, vuln + '_patches.csv'))\n",
    "\n",
    "        # iterate patch_link and original_link from the patches using zip\n",
    "        for patch_link, original_link in zip(patches_vuln['patch_link'], patches_vuln['original_link']):\n",
    "            # patch_path and og_path are functions that replace the prefix of the path\n",
    "            patch = patch_path(patch_link)\n",
    "            original = og_path(original_link)\n",
    "\n",
    "            #  check if patch and original exist\n",
    "            if not os.path.exists(patch):\n",
    "                print(f\"Patch file does not exist: {patch}\")\n",
    "                # continue\n",
    "            if not os.path.exists(original):\n",
    "                print(f\"Original file does not exist: {original}\")\n",
    "                # continue\n",
    "\n",
    "            # diff_files is a function that returns the diff between two files\n",
    "            diff = diff_files(original, patch, original_link, patch_link)\n",
    "\n",
    "            filename = patch.replace('.sol', '.diff')\n",
    "            print(\"Filename: \", filename)\n",
    "\n",
    "            # Save the diff to a file\n",
    "            with open(filename, 'w') as f:\n",
    "                f.write(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_diffs_for_all(force_regenerate=True):\n",
    "    \"\"\"\n",
    "    Generate diff files for all source code patches in all_patches_stats.csv \n",
    "    Places diff files in the same directory as each patch file.\n",
    "    Uses the existing patch_path, og_path, and diff_files functions.\n",
    "    \n",
    "    Args:\n",
    "        force_regenerate (bool): If True, regenerate even if diff file exists\n",
    "    \"\"\"\n",
    "    all_patches_stats = pd.read_csv(patches_path)\n",
    "\n",
    "\n",
    "    print(f\"Processing {len(all_patches_stats)} patches for diff generation...\")\n",
    "    if force_regenerate:\n",
    "        print(\"⚠️  Force regeneration enabled - will replace existing diff files\")\n",
    "    \n",
    "    stats = {\n",
    "        'total': len(all_patches_stats),\n",
    "        'generated': 0,\n",
    "        'replaced': 0,\n",
    "        'skipped_existing': 0,\n",
    "        'skipped_missing_files': 0,\n",
    "        'errors': 0\n",
    "    }\n",
    "    \n",
    "    # Iterate through all patches in all_patches_stats\n",
    "    for idx, row in all_patches_stats.iterrows():\n",
    "        patch_link = row['patch_link']\n",
    "        original_link = row['original_link']\n",
    "        \n",
    "        try:\n",
    "            # Convert GitHub URLs to local paths using existing functions\n",
    "            patch = patch_path(patch_link)\n",
    "            original = og_path(original_link)\n",
    "            \n",
    "            # Skip bytecode files (.bin, .hex) since we don't have source code to compare\n",
    "            patch_basename = os.path.basename(patch)\n",
    "            patch_ext = os.path.splitext(patch_basename)[1].lower()\n",
    "            \n",
    "            if patch_ext in ['.bin', '.hex']:\n",
    "                print(f\"Skipping bytecode file: {patch_basename}\")\n",
    "                stats['skipped_missing_files'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Create diff filename in the same directory as the patch file\n",
    "            patch_dir = os.path.dirname(patch)\n",
    "            \n",
    "            # Remove extension and add .diff\n",
    "            diff_basename = os.path.splitext(patch_basename)[0] + '.diff'\n",
    "            diff_filename = os.path.join(patch_dir, diff_basename)\n",
    "            \n",
    "            # Check if diff already exists\n",
    "            if os.path.exists(diff_filename):\n",
    "                if force_regenerate:\n",
    "                    print(f\"Replacing existing diff: {diff_basename}\")\n",
    "                    stats['replaced'] += 1\n",
    "                else:\n",
    "                    print(f\"Diff already exists, skipping: {diff_filename}\")\n",
    "                    stats['skipped_existing'] += 1\n",
    "                    continue\n",
    "            \n",
    "            # Check if source files exist\n",
    "            missing_files = []\n",
    "            if not os.path.exists(patch):\n",
    "                missing_files.append(f\"Patch: {patch}\")\n",
    "            if not os.path.exists(original):\n",
    "                missing_files.append(f\"Original: {original}\")\n",
    "            \n",
    "            if missing_files:\n",
    "                print(f\"Missing files for row {idx}:\")\n",
    "                for missing in missing_files:\n",
    "                    print(f\"  - {missing}\")\n",
    "                stats['skipped_missing_files'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Generate the diff using existing function (fix parameter order)\n",
    "            print(f\"Generating diff for: {patch_basename} -> {diff_basename}\")\n",
    "            diff_content = diff_files(original, patch, original_link, patch_link)\n",
    "            \n",
    "            # Ensure patch directory exists (should already exist if patch file exists)\n",
    "            if not os.path.exists(patch_dir):\n",
    "                os.makedirs(patch_dir, exist_ok=True)\n",
    "            \n",
    "            # Save the diff to file in the same directory as the patch\n",
    "            with open(diff_filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(diff_content)\n",
    "            \n",
    "            if not os.path.exists(diff_filename) or stats['replaced'] == 0:\n",
    "                stats['generated'] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            print(f\"  Patch link: {patch_link}\")\n",
    "            print(f\"  Original link: {original_link}\")\n",
    "            stats['errors'] += 1\n",
    "            continue\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\n=== Diff Generation Summary ===\")\n",
    "    print(f\"Total patches processed: {stats['total']}\")\n",
    "    print(f\"New diffs generated: {stats['generated']}\")\n",
    "    print(f\"Existing diffs replaced: {stats['replaced']}\")\n",
    "    print(f\"Already existing (skipped): {stats['skipped_existing']}\")\n",
    "    print(f\"Missing source files (skipped): {stats['skipped_missing_files']}\")\n",
    "    print(f\"Errors encountered: {stats['errors']}\")\n",
    "    \n",
    "    total_created = stats['generated'] + stats['replaced']\n",
    "    if total_created > 0:\n",
    "        print(f\"\\n✅ Successfully created/replaced {total_created} diff files!\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️  No diff files were generated or replaced.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all diffs, replacing existing ones (recommended for your situation)\n",
    "#generate_diffs_for_all(force_regenerate=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
